% !TEX program = pdflatex
\documentclass[11pt,a4paper]{article}

% -----------------------
% Encoding + language
% -----------------------
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% -----------------------
% Scalable fonts (fixes pdfTeX microtype expansion issues)
% -----------------------
\usepackage{lmodern}

% -----------------------
% Typography (safe microtype config)
% -----------------------
\usepackage[final]{microtype}
% If your setup still complains, uncomment the next line:
% \microtypesetup{expansion=false}

% -----------------------
% Page layout
% -----------------------
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{subcaption}


\usepackage{setspace}
\onehalfspacing

% -----------------------
% Math + symbols
% -----------------------
\usepackage{amsmath,amssymb}

% -----------------------
% Figures + tables
% -----------------------
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}

% -----------------------
% Links
% -----------------------
\usepackage[hidelinks]{hyperref}

% -----------------------
% Code listings (optional)
% -----------------------
\usepackage{xcolor}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  columns=fullflexible,
  showstringspaces=false
}

% -----------------------
% Nice lists
% -----------------------
\usepackage{enumitem}
\setlist{noitemsep, topsep=0.3em}

% -----------------------
% Title meta
% -----------------------
\title{Radar Visualization Pipeline Report}
\author{Aiysha Mei Frutiger, Sandro Barbazza, Senanur Ates,  \\ University of Basel \\ Computer Architecture}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Abstract of project
\end{abstract}

\tableofcontents
\newpage

% ==========================================================
\section{Introduction}
We built a compact ultrasonic ``radar-style'' scanner that turns echo timing into a live 2D visualization. 
An ultrasonic pulse is emitted and the return echo duration encodes time-of-flight, which the microcontroller converts to distance. 
By sweeping the sensor with a servo, each measurement becomes an $(\textit{angle}, \textit{distance})$ pair that can be rendered in polar form.
The project is primarily an educational end-to-end system that makes the hardware--software stack observable: sensing, embedded processing, USB serial transport, OS I/O abstraction, and real-time visualization.

% ==========================================================
\section{Planning}

% ==========================================================
\section{Hardware}

% ==========================================================
\section{Servo Sweep Control}
% ==========================================================
\section{System Overview: From Echo to Visualization}\label{sec:system-overview}

Figure~\ref{fig:pipeline} summarizes the end-to-end pipeline. The ultrasonic sensor returns an
\emph{echo pulse width}, i.e., a time duration proportional to the time-of-flight.
The Arduino controls the servo sweep via PWM, triggers the sensor, measures the echo duration
(e.g., with \texttt{pulseIn}), converts it to a distance in centimeters, and transmits one
newline-terminated ASCII record per measurement (\texttt{angle,cm}).
Over USB the payload is a byte stream which the operating system exposes as a serial device
(e.g., a COM port on Windows). The Processing program opens this port, frames the incoming bytes
into lines, parses angle and distance, and updates the visualization state used for rendering.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.90\linewidth]{images/Pipeline.png}
  \caption{End-to-end pipeline from sensing on the Arduino to visualization on the host.}
  \label{fig:pipeline}
\end{figure}

% ==========================================================
\section{Visualization}\label{sec:visualization}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{images/finalVisual.png}
  \caption{Final Processing-based radar visualization (grid, sweep line, and detections).}
  \label{fig:final-visual}
\end{figure}

The goal of the visualization is to make streamed sensor measurements immediately interpretable as a
radar-style scan. The display renders a half-circle field of view ($0^\circ$--$180^\circ$) with a moving
sweep line representing the current scan direction. Each incoming measurement $(\textit{angle},\textit{distance})$
is mapped to polar coordinates: the angle determines the direction of the sweep and the distance
determines the radius of a ``blip'' point. A static background grid (concentric arcs and radial dividers)
provides scale and orientation (Figure~\ref{fig:final-visual}).

\subsection{Implementation path}
We initially prototyped the UI in C using \texttt{raylib}. While rendering worked well, the end-to-end
path was unstable due to low-level serial I/O handling on Windows (port selection, timing after reset,
partial lines, and port contention). To reduce OS-specific complexity and iterate faster, we switched to
Processing, whose serial library directly supports line-based framing and event-driven reads.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.60\linewidth]{images/fistPrototype.png}
  \caption{C/\texttt{raylib} prototype in simulated sweep mode (no robust sensor-driven end-to-end path).}
  \label{fig:raylib-sim}
\end{figure}

\subsection{Serial parsing and ``bucket'' state}
Processing treats the serial connection as a continuous byte stream but frames it into complete records
using newline termination. Each line is trimmed, split by comma, and parsed into numeric values
(\textit{angle} and \textit{distance}). Invalid readings are encoded as \texttt{-1} and interpreted as
``no detection''.

To prevent unbounded accumulation of points and to mimic a radar refresh, the scan area is discretized
into small angle slices (``buckets'') of fixed width (e.g., \texttt{ANGLE\_BUCKET\_DEG}). Each bucket stores
the most recent distance for that slice and is overwritten when the sweep returns. If a \texttt{-1} reading
arrives for a slice, the corresponding bucket is cleared. This keeps memory bounded and creates the
intuitive behavior that detections persist briefly and disappear naturally on rescan.

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images/secondPrototype.png}
    \caption{Early Processing iteration used to validate coordinate mapping and sweep direction.}
    \label{fig:proc-early}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images/secondPrototypeFade.png}
    \caption{Intermediate iteration with a fading persistence effect per bucket.}
    \label{fig:proc-fade}
  \end{subfigure}
  \caption{Processing visualization iterations during development.}
  \label{fig:proc-evolution}
\end{figure}


==========================================================

\section{Conclusion}
We implemented an end-to-end ultrasonic ``radar-style'' system that turns echo time-of-flight into a live 2D
visualization. This project made the full stack tangible: the Arduino converts timing signals into structured
serial records, USB transports them as bytes, the operating system exposes the device as a serial port, and
Processing frames, parses, and renders the stream into an interpretable scan display.

A key lesson was building robustness across layers and constraints. We observed practical serial-port behavior
(e.g., exclusivity and startup noise) and designed the software to handle imperfect input. Since the servo was
not available during early visualization work, the Processing program supports both a simulated sweep and a
servo-driven angle mode, switching automatically when angle data is present.

\paragraph{Division of labor.}
All group members contributed to assembling the hardware setup. Sandro implemented the main Arduino sensing
and communication code and built the stand/chassis. Senanur developed the Arduino servo sweep control. Aiysha
implemented the Processing visualization, including parsing and bucket-based rendering.

==========================================================

\section{Declaration of Authorship}

ChatGPT was used solely to improve the clarity and coherence of the report's language. All ideas, analyses, 
and interpretations presented reflect the group's own work and research.


\end{document}
